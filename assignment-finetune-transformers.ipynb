{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:12:03.748971Z","iopub.execute_input":"2025-09-07T16:12:03.749310Z","iopub.status.idle":"2025-09-07T16:12:05.295722Z","shell.execute_reply.started":"2025-09-07T16:12:03.749284Z","shell.execute_reply":"2025-09-07T16:12:05.294973Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom sklearn.metrics import f1_score, accuracy_score\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n    set_seed,\n    pipeline\n)\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:12:31.818967Z","iopub.execute_input":"2025-09-07T16:12:31.819613Z","iopub.status.idle":"2025-09-07T16:12:59.770618Z","shell.execute_reply.started":"2025-09-07T16:12:31.819588Z","shell.execute_reply":"2025-09-07T16:12:59.769865Z"}},"outputs":[{"name":"stderr","text":"2025-09-07 16:12:46.476679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757261566.651810      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757261566.704607      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Settings\nseed = 42\nset_seed(seed)\nsubset_size = 3000\nbatch_size = 16\ncompare_epochs = 2\nfull_epochs = 2\n\nmodels_to_try = [\n\"bert-base-uncased\",\n\"roberta-base\",\n\"google/electra-small-discriminator\",\n\"distilbert-base-uncased\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:13:06.360055Z","iopub.execute_input":"2025-09-07T16:13:06.360994Z","iopub.status.idle":"2025-09-07T16:13:06.369818Z","shell.execute_reply.started":"2025-09-07T16:13:06.360969Z","shell.execute_reply":"2025-09-07T16:13:06.369020Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load dataset\ndef load_imdb():\n    try:\n        df = pd.read_csv(\"IMDB Dataset.csv\")\n        df[\"label\"] = df[\"sentiment\"].map({\"negative\": 0, \"positive\": 1})\n    except FileNotFoundError:\n        ds = load_dataset(\"imdb\")\n        df = pd.concat([\n            pd.DataFrame({\"review\": ds[\"train\"][\"text\"], \"label\": ds[\"train\"][\"label\"]}),\n            pd.DataFrame({\"review\": ds[\"test\"][\"text\"], \"label\": ds[\"test\"][\"label\"]})\n        ]).sample(frac=1, random_state=seed).reset_index(drop=True)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:13:10.904385Z","iopub.execute_input":"2025-09-07T16:13:10.904991Z","iopub.status.idle":"2025-09-07T16:13:10.910277Z","shell.execute_reply.started":"2025-09-07T16:13:10.904965Z","shell.execute_reply":"2025-09-07T16:13:10.909439Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Split dataset\ndef make_dataset(df, n):\n    df = df.sample(n=n, random_state=seed).reset_index(drop=True)\n    n_train = int(0.8*n)\n    n_val = int(0.1*n)\n    return DatasetDict({\n        \"train\": Dataset.from_pandas(df.iloc[:n_train]),\n        \"validation\": Dataset.from_pandas(df.iloc[n_train:n_train+n_val]),\n        \"test\": Dataset.from_pandas(df.iloc[n_train+n_val:])\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:13:12.958454Z","iopub.execute_input":"2025-09-07T16:13:12.958742Z","iopub.status.idle":"2025-09-07T16:13:12.963676Z","shell.execute_reply.started":"2025-09-07T16:13:12.958719Z","shell.execute_reply":"2025-09-07T16:13:12.962823Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Tokenize with progress bar\ndef tokenize_with_progress(ds, tokenizer):\n    return ds.map(\n        lambda x: tokenizer(x[\"review\"], truncation=True),\n        batched=True,\n        remove_columns=[\"review\"],\n        desc=\"Tokenizing dataset\"\n    )\n\n# Custom metric\ndef compute_metrics(pred):\n    preds = np.argmax(pred.predictions, axis=1)\n    labels = pred.label_ids\n    return {\n        \"f1\": f1_score(labels, preds),\n        \"accuracy\": accuracy_score(labels, preds)\n    }\n# Quick compare\ndf_all = load_imdb()\nds_small = make_dataset(df_all, subset_size)\nresults = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:13:25.200942Z","iopub.execute_input":"2025-09-07T16:13:25.201705Z","iopub.status.idle":"2025-09-07T16:13:30.781398Z","shell.execute_reply.started":"2025-09-07T16:13:25.201681Z","shell.execute_reply":"2025-09-07T16:13:30.780799Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d349db2b13c046aea5791c9ba625df71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f040456b2ede4044b45267f51f5b0768"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b6ac217c0e43558716892003a2aa6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/unsupervised-00000-of-00001.p(â€¦):   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95182828922e491cbacadfd9d265827b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4fbd231e92b42c893c3b78a6479294e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6429ee4e4a64e0ea59aae7265492634"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4573565c1c7d4966aa2a9cbc515cdb43"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"for model_name in models_to_try:\n    print(f\"Training {model_name}...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenized = tokenize_with_progress(ds_small, tokenizer)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n    trainer = Trainer(\n        model=model,\n        args=TrainingArguments(\n        output_dir=f\"./{model_name}-compare\",\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=compare_epochs,\n        logging_steps=10,              # log more frequently\n        logging_first_step=True,       # log the very first step\n        report_to=\"none\",              # disable W&B/MLflow noise\n        save_total_limit=1\n    ),\n\n        train_dataset=tokenized[\"train\"],\n        eval_dataset=tokenized[\"validation\"],\n        tokenizer=tokenizer,\n        data_collator=DataCollatorWithPadding(tokenizer),\n        compute_metrics=compute_metrics\n    )\n    trainer.train()\n    eval_metrics = trainer.evaluate(tokenized[\"validation\"])\n    results.append({\"model\": model_name, \"f1\": eval_metrics[\"eval_f1\"]})\n    del model\n    torch.cuda.empty_cache()\n\nbest_model = max(results, key=lambda x: x[\"f1\"])[\"model\"]\nprint(\"Best model:\", best_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:13:37.718407Z","iopub.execute_input":"2025-09-07T16:13:37.718706Z","iopub.status.idle":"2025-09-07T16:26:16.952362Z","shell.execute_reply.started":"2025-09-07T16:13:37.718679Z","shell.execute_reply":"2025-09-07T16:26:16.951630Z"}},"outputs":[{"name":"stdout","text":"Training bert-base-uncased...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8362341ae65643afbf744860406080a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90225dd5af084ff6b7097ff7ab698cd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2410988dad4f618fe36c32bbc775ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fc65f193b3142cdaa2a0211e0322202"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/2400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a92e62be784cbd8cabb90f3f93c775"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38cd876d78a54c4bb9af09786e11bfd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fd518663af43c0af7120040d009220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c3c60e4d874bf7a5af605de38f295b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4191990080.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 04:20, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.666900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.677500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.561200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.329600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.293900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.284000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.235500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.442300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.224900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.199000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.167700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.120900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.135700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.125200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.108200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training roberta-base...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6452f617cf6443b18d20493ddddefc0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f9bf583902748538457ca069205f418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a18ce435bf47ad9e71c8361d17f999"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dcd1edbf88948979db0ed2daf5ed248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d77f0e14e24a228d60bab436921bbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/2400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddefcd5fb89045fd897d680a36f89c28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"382b4839472e482294740f9d2785a700"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd527613eae14dc9a31bf9dbe3a84fde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ae4cf11edda4084932e19c26fc2d76b"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4191990080.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 04:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.730200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.677700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.445900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.271400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.248400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.305700</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.209600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.413300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.149700</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.168500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.147300</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.108200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.196000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.132300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.112600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.120300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training google/electra-small-discriminator...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4dd07ea1f7d4959a13f41c494ee6b45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1deade8aba44edca3cb1d0fb57fb82f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8bc90ec050f43918956d2b90225944c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3de8ec68a7e4355b695ddb7ae5ccef8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/2400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b426081003974b9782e6ac00f7c93d1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efa3c90613d6465a82ed304aa3969b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d131d004b0314a96a294983213b7a58b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8b8955de89c48979598f1e3abfbaf6b"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4191990080.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/54.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75d8951020ea4dd0b400bd7b33a22c97"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 00:52, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.691000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.691900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.688200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.679200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.633800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.581400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.550800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.536700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.468300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.454400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.436800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.424400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.398300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.397800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.369000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.370000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Training distilbert-base-uncased...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b34bf4a5602c4b7ab125438db3bfa46d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a3994e07644dc9936f598739c54080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea66685afa504bd0bc332e085f38ce76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"666c6a60910e4869b5f9fc8dc345534c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/2400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6efb80c08a2b41b395ebfd3883b915ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d734032069640abb5a699a3ee01e742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7921e43b605a4542a2ca703852af1fe8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fdc6e172c0941808a9597ef79206b19"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4191990080.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 02:14, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.678400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.691100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.543100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.330300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.330700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.388400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.315400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.482300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.260900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.215600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.204000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.226700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.172300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.216600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.173400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.177600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Best model: roberta-base\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Full training\ndf = df_all.sample(frac=1, random_state=seed).reset_index(drop=True)\nn = len(df)\ntrain_end, val_end = int(0.8*n), int(0.9*n)\nfull_ds = DatasetDict({\n\"train\": Dataset.from_pandas(df.iloc[:train_end]),\n\"validation\": Dataset.from_pandas(df.iloc[train_end:val_end]),\n\"test\": Dataset.from_pandas(df.iloc[val_end:])\n})\n\n\ntokenizer = AutoTokenizer.from_pretrained(best_model)\nfull_tokenized = tokenize_with_progress(full_ds, tokenizer)\nmodel = AutoModelForSequenceClassification.from_pretrained(best_model, num_labels=2)\n\ntrainer = Trainer(\n    model=model,\n    args=TrainingArguments(\n        output_dir=f\"./{best_model}-full\",\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=full_epochs,   # use full_epochs, not compare_epochs\n        logging_steps=10,\n        logging_first_step=True,\n        report_to=\"none\",\n        save_total_limit=1\n    ),\n    train_dataset=full_tokenized[\"train\"],\n    eval_dataset=full_tokenized[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorWithPadding(tokenizer),\n    compute_metrics=compute_metrics\n)\n\n\n\ntrainer.train()\nprint(\"Final evaluation:\", trainer.evaluate(full_tokenized[\"test\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T16:30:45.452686Z","iopub.execute_input":"2025-09-07T16:30:45.453479Z","iopub.status.idle":"2025-09-07T17:47:00.518189Z","shell.execute_reply.started":"2025-09-07T16:30:45.453440Z","shell.execute_reply":"2025-09-07T17:47:00.517537Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98be4346909d4942bd4a3828245c5020"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2867bef6742a49b0beae5704260e84e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e5cc2f6a193432a8ee1336589e9f66f"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/165009429.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2500/2500 1:14:21, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.724200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.719100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.434400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.371800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.266000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.433200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.270700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.310500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.235500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.262300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.323400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.248200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.195900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.232900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.231400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.239300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.290400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.263400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.308000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.273100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.246100</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.223700</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.271400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.123800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.243300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.159700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.409000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.202300</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.217500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.177100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.179400</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.293700</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.213800</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.250900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.203100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.255700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.220200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.222200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.171900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.172000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.239400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.252100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.240000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.209700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.186200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.203600</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.192000</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.202100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.209700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.192300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.242000</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.209500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.255200</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.205800</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.150400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.184000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.182600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.149800</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.158900</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.234500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.171200</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.120500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.219300</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.256500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.161400</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.164700</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.196500</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.215400</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.225500</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.082200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.301900</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.418000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.268400</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.221900</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.165600</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.180200</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.213700</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.162500</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.184000</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.144400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.238300</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.136900</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.116900</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.181500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.186300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.201100</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.141100</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.238700</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.225900</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.121100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.145300</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.124900</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.141300</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.170900</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.134800</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.188200</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.190200</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.162900</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.140200</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.185100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.226700</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.251600</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.184000</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.144900</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.128700</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.144900</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.172600</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.123400</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.149400</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.128400</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.225800</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.152500</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.189000</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.160300</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.144700</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.188200</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.273600</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.139000</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.102000</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.135200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.262200</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.285800</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.202100</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.233100</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.139700</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.134300</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.062600</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.104200</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.123300</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.090700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.131600</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.111500</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.098700</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.102700</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.102900</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.180900</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.103500</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.110700</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.126100</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.085800</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.183500</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.124200</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.082900</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.085400</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.100700</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.114500</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.089200</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.144000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.143100</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.082600</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.191100</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.103100</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.114800</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.063100</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.091700</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.072900</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.177500</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.070900</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.039300</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.111400</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.101500</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.150600</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.118700</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.099500</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.079400</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.074600</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.057400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.087200</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.123800</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.076300</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.089400</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.129600</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.095200</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.084700</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.096500</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.108100</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.095800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.089300</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.082100</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.064700</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.103900</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.060700</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.100500</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.067700</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.113700</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.126500</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.098500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.154300</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.118300</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.089200</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.081700</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.105300</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.099700</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.097400</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.118700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.106200</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.097400</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.118000</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.139100</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.092200</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.093400</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.117000</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.048900</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.069200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.082300</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.181400</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.081800</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.095400</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.095800</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.085300</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.171600</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.096900</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.112400</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.089200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.051800</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.075700</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.114000</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.109700</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.143800</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.100300</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.115200</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.147000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.186600</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.053100</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.089900</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.068200</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.171400</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.081800</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.095000</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.088100</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.115500</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.072100</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.071500</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.055900</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.059600</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.094100</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.065900</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.111300</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.077700</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.112900</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.156000</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.106200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.101800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [157/157 01:27]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final evaluation: {'eval_loss': 0.15752272307872772, 'eval_f1': 0.9549007817197835, 'eval_accuracy': 0.955, 'eval_runtime': 88.3111, 'eval_samples_per_second': 56.618, 'eval_steps_per_second': 1.778, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Inference\npipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\nsample = df.iloc[val_end:].sample(10, random_state=seed)\n\nprint(\"\\nRunning inference on 10 samples...\\n\")\nfor _, row in tqdm(sample.iterrows(), total=len(sample), desc=\"Inference\"):\n    pred = pipe(row[\"review\"][:1000])\n    print(\"Review:\", row[\"review\"][:200].replace(\"\\n\", \" \"))\n    print(\"True:\", row[\"label\"], \"Pred:\", pred, \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T17:48:37.672838Z","iopub.execute_input":"2025-09-07T17:48:37.673455Z","iopub.status.idle":"2025-09-07T17:48:37.955833Z","shell.execute_reply.started":"2025-09-07T17:48:37.673431Z","shell.execute_reply":"2025-09-07T17:48:37.954913Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\nRunning inference on 10 samples...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Inference:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11cb4609685a4545b70cb170ea5e638a"}},"metadata":{}},{"name":"stdout","text":"Review: Had this been the original 1914 version of TESS OF THE STORM COUNTRY (also starring Mary Pickford), I probably would have rated it a lot higher, as this sort of extreme melodrama and sentimentality wa\nTrue: 1 Pred: [{'label': 'LABEL_1', 'score': 0.9916493892669678}] \n\nReview: If you have trouble suspending disbelief then this isn't for you. Consider: a woman already in late middle age finds a newborn baby in a cabbage patch and raises it as her own. Think about it; she mak\nTrue: 1 Pred: [{'label': 'LABEL_0', 'score': 0.7532424926757812}] \n\nReview: This movie was packed pull of endless surprises! Just when you thought it couldn't get worse, they added more joints and more pink fuzzy-lined vans with raunchy sex scenes. As you can guess, I was a v\nTrue: 0 Pred: [{'label': 'LABEL_0', 'score': 0.8433667421340942}] \n\nReview: San Francisco is a big city with great acting credits. In this one, the filmmakers made no attempt to use the city. They didn't even manage the most basic of realistic details. So I would not recommen\nTrue: 0 Pred: [{'label': 'LABEL_0', 'score': 0.9864776134490967}] \n\nReview: This movie is not for those expecting a martial-arts extravaganza. As for historical accuracy, I will leave that issue to those better informed on that subject. I thought the performances were wonderf\nTrue: 1 Pred: [{'label': 'LABEL_1', 'score': 0.997261643409729}] \n\nReview: This was it! i would have never expected the ending if i didn't already know the behind the scenes stuff.<br /><br />The one thing that i hated was that why was Shannon kicked off and not Alyssa. i ha\nTrue: 1 Pred: [{'label': 'LABEL_1', 'score': 0.996855616569519}] \n\nReview: When I remember seeing the previews for this movie and not really thinking much about it. It was almost one of those movies that when you see the preview, its stunning, and then when it comes out, you\nTrue: 1 Pred: [{'label': 'LABEL_1', 'score': 0.9970675110816956}] \n\nReview: My introduction into Yoji Yamada's cinematic world is through his famed and recent Samurai Trilogy with The Twilight Samurai, The Hidden Blade and Love and Honor. I had enjoyed all three films, and lo\nTrue: 1 Pred: [{'label': 'LABEL_1', 'score': 0.9953669309616089}] \n\nReview: River's Edge is an extremely disturbing film written by acclaimed American screen writer Neal Jimenez.It is based on an actual event which happened at a time when most of American youngsters were tryi\nTrue: 1 Pred: [{'label': 'LABEL_1', 'score': 0.9971404075622559}] \n\nReview: The Elegant Documentary -<br /><br />Don't watch this movie ... if you're an egotistical know-all student of physics. This much less than one percent (miniscule fraction) of the population may find th\nTrue: 1 Pred: [{'label': 'LABEL_1', 'score': 0.9749870300292969}] \n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}