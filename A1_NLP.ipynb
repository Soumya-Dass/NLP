{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofU7Y7k1LHQw",
        "outputId": "67a8a6a8-085f-46fd-ff3c-8dc7f6455372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load Google News pretrained model\n",
        "model = api.load('word2vec-google-news-300')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3eHOOVlLJgL",
        "outputId": "208fbc96-5200-46b4-d07a-8782b6991b13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['apple', 'car', 'university', 'computer', 'music']\n",
        "for word in words:\n",
        "    print(f\"\\nTop similar words for '{word}':\")\n",
        "    similar = model.most_similar(word, topn=5)\n",
        "    for sim_word, score in similar:\n",
        "        print(f\"{sim_word}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv0UtRuYNCFo",
        "outputId": "922ed5cc-90e0-477a-a4d1-9b4d254a29ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top similar words for 'apple':\n",
            "apples: 0.7204\n",
            "pear: 0.6451\n",
            "fruit: 0.6410\n",
            "berry: 0.6302\n",
            "pears: 0.6134\n",
            "\n",
            "Top similar words for 'car':\n",
            "vehicle: 0.7821\n",
            "cars: 0.7424\n",
            "SUV: 0.7161\n",
            "minivan: 0.6907\n",
            "truck: 0.6736\n",
            "\n",
            "Top similar words for 'university':\n",
            "universities: 0.7004\n",
            "faculty: 0.6781\n",
            "unversity: 0.6758\n",
            "undergraduate: 0.6587\n",
            "univeristy: 0.6585\n",
            "\n",
            "Top similar words for 'computer':\n",
            "computers: 0.7979\n",
            "laptop: 0.6640\n",
            "laptop_computer: 0.6549\n",
            "Computer: 0.6473\n",
            "com_puter: 0.6082\n",
            "\n",
            "Top similar words for 'music':\n",
            "classical_music: 0.7198\n",
            "jazz: 0.6835\n",
            "Music: 0.6596\n",
            "Without_Donny_Kirshner: 0.6416\n",
            "songs: 0.6396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))  # â‰ˆ queen\n",
        "print(model.most_similar(positive=['paris', 'italy'], negative=['france'], topn=1))  # â‰ˆ rome\n",
        "print(model.most_similar(positive=['walk', 'swimming'], negative=['walked'], topn=1))  # â‰ˆ swim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQlx80weOdKb",
        "outputId": "c68c3aea-452e-4822-8833-874e9ecfea35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519)]\n",
            "[('lohan', 0.5069674849510193)]\n",
            "[('swim', 0.7891075611114502)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRWVisAxOjcr",
        "outputId": "1f3cfa4f-a074-47b0-e40a-7b74310829ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n"
      ],
      "metadata": {
        "id": "xcwoleUcO9e2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [w for w in tokens if w not in stop_words and len(w) > 1]\n",
        "    return tokens\n",
        "\n",
        "df['cleaned'] = df['review'].apply(clean_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7GkvkrqPKll",
        "outputId": "8200c1f9-5cb1-4fa3-fe9a-8ebc99cfa024"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_avg_vector(tokens, model, dim=300):\n",
        "    valid = [token for token in tokens if token in model]\n",
        "    if valid:\n",
        "        return np.mean([model[token] for token in valid], axis=0)\n",
        "    else:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "X = np.array([get_avg_vector(tokens, model) for tokens in df['cleaned']])\n",
        "y = (df['sentiment'] == 'positive').astype(int)\n"
      ],
      "metadata": {
        "id": "LDnZXbm-PL8Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "print(classification_report(y_test, clf.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od55XQOQRVLe",
        "outputId": "be2e5a02-b48b-42a8-ce82-fea13ae6dfb4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.81      0.81      4961\n",
            "           1       0.81      0.82      0.81      5039\n",
            "\n",
            "    accuracy                           0.81     10000\n",
            "   macro avg       0.81      0.81      0.81     10000\n",
            "weighted avg       0.81      0.81      0.81     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "skipgram_model = Word2Vec(sentences=df['cleaned'], vector_size=100, window=5, sg=1, min_count=5, workers=4)\n"
      ],
      "metadata": {
        "id": "3Ce3rV25RccX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model = Word2Vec(sentences=df['cleaned'], vector_size=100, window=5, sg=0, min_count=5, workers=4)\n"
      ],
      "metadata": {
        "id": "xEhDAtiuSLnu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "fasttext_model = FastText(sentences=df['cleaned'], vector_size=100, window=5, min_count=5, workers=4)\n"
      ],
      "metadata": {
        "id": "aJYEkNzrTHIi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 1. Function to get average word vectors per document\n",
        "def get_avg_vector(tokens, wv_model, dim):\n",
        "    valid_tokens = [word for word in tokens if word in wv_model]\n",
        "    if not valid_tokens:\n",
        "        return np.zeros(dim)\n",
        "    return np.mean([wv_model[word] for word in valid_tokens], axis=0)\n",
        "\n",
        "# 2. Evaluation function\n",
        "def evaluate_model(wv_model, model_name, dim=100):\n",
        "    print(f\"Evaluating: {model_name}\")\n",
        "\n",
        "    X = np.array([get_avg_vector(tokens, wv_model.wv, dim) for tokens in df['cleaned']])\n",
        "    y = (df['sentiment'] == 'positive').astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    clf = RandomForestClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': report['weighted avg']['precision'],\n",
        "        'Recall': report['weighted avg']['recall'],\n",
        "        'F1 Score': report['weighted avg']['f1-score']\n",
        "    }\n",
        "\n",
        "# 3. Run for all 3 models\n",
        "results = []\n",
        "\n",
        "results.append(evaluate_model(skipgram_model, \"Custom Skip-gram\"))\n",
        "results.append(evaluate_model(cbow_model, \"Custom CBOW\"))\n",
        "results.append(evaluate_model(fasttext_model, \"Custom FastText\"))\n",
        "\n",
        "# 4. Create summary table\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nðŸ“Š Summary Table:\")\n",
        "print(results_df)\n",
        "\n",
        "# Optional: Save to CSV\n",
        "results_df.to_csv(\"word_vector_model_summary.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFMoO017TZtI",
        "outputId": "22400f14-9ca6-4df4-f29f-bd075b7f4b7f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating: Custom Skip-gram\n",
            "Evaluating: Custom CBOW\n",
            "Evaluating: Custom FastText\n",
            "\n",
            "ðŸ“Š Summary Table:\n",
            "              Model  Accuracy  Precision  Recall  F1 Score\n",
            "0  Custom Skip-gram    0.8470   0.847232  0.8470  0.846954\n",
            "1       Custom CBOW    0.8375   0.837985  0.8375  0.837410\n",
            "2   Custom FastText    0.8235   0.824073  0.8235  0.823383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jlnUGZ_sUxVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}